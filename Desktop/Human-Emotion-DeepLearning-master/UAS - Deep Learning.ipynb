{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2e951a-8f84-4be3-89e5-13bad6ff5c7c",
   "metadata": {},
   "source": [
    "Membangun model klasifikasi emosi wajah berbasis deep learning dengan menggunakan dataset yang telah dikelompokkan ke dalam folder untuk masing-masing emosi. Dataset diolah menggunakan augmentasi data untuk memperluas variasi data pelatihan, sehingga model menjadi lebih robust.\n",
    "\n",
    "Dataset dan Emosi: Dataset citra wajah dipisahkan ke dalam folder train (untuk pelatihan) dan test (untuk pengujian). Kategori emosi yang akan diklasifikasikan didefinisikan dalam daftar emotions, misalnya 'angry', 'neutral', dll.\n",
    "\n",
    "Augmentasi Data: Augmentasi diterapkan pada data pelatihan dengan menggunakan ImageDataGenerator untuk melakukan operasi seperti rescaling (normalisasi piksel), shear, zoom, dan flipping horizontal. Data pengujian hanya dinormalisasi tanpa augmentasi.\n",
    "\n",
    "Generator Data: flow_from_directory digunakan untuk secara otomatis membaca dataset dari folder, mengubah ukuran gambar menjadi 48x48 piksel (sesuai dengan dimensi model), dan mengubahnya menjadi batch yang siap diproses oleh model.\n",
    "\n",
    "Fungsi: Proses ini memastikan bahwa model CNN akan menerima data dalam format yang sesuai, dengan augmentasi yang membantu meningkatkan performa model pada data baru, sementara data pengujian digunakan untuk mengevaluasi akurasi model pada data yang tidak dilatih."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08dfbdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57544 images belonging to 7 classes.\n",
      "Found 14244 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "train_dir = './train'\n",
    "test_dir = './test'\n",
    "\n",
    "# Define the emotions\n",
    "emotions = ['angry','neutral', 'disgust', 'fear', 'happy','sad', 'surprise']\n",
    "\n",
    "# Define the image dimensions\n",
    "img_height, img_width = 48, 48\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=(img_height, img_width),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=32,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77daaa51-276e-40be-9c30-be28b8c5cae5",
   "metadata": {},
   "source": [
    "**Membangun Model CNN**\n",
    "\n",
    "Pelatihan model Convolutional Neural Network (CNN) untuk klasifikasi emosi wajah berdasarkan gambar. Model ini menggunakan TensorFlow dan Keras, memanfaatkan GPU untuk mempercepat pelatihan. Setelah mendeteksi GPU yang tersedia, model dibangun dengan beberapa lapisan convolutional untuk mengekstraksi fitur gambar, diikuti dengan lapisan pooling untuk mereduksi dimensi data. Setelah fitur diekstraksi, data diratakan dan diteruskan ke lapisan fully connected (Dense) untuk membuat prediksi akhir. Fungsi aktivasi relu digunakan untuk non-linearitas pada lapisan convolutional, sementara softmax digunakan pada lapisan output untuk menghasilkan distribusi probabilitas atas 7 kelas emosi. Model dikompilasi dengan optimizer Adam, loss function categorical_crossentropy, dan metrik akurasi. Pelatihan dilakukan selama 70 epoch menggunakan data training yang dihasilkan dari generator dengan augmentasi gambar, sedangkan data validasi digunakan untuk memantau performa selama pelatihan. Akhirnya, model dievaluasi pada data testing untuk mendapatkan akurasi akhir.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d6bdb9-6221-4bc8-b85c-d5864b7e8289",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Set the GPU as the default device\n",
    "with tf.device('/GPU:0'):\n",
    "    # Build the model\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(len(emotions), activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(train_generator,\n",
    "                        epochs=70,\n",
    "                        validation_data=test_generator)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(test_generator)\n",
    "    print(f'Test accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed74a146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('emotion_detection_model_CNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cbc421-73f7-472c-8f7a-59ffed0e073c",
   "metadata": {},
   "source": [
    "**Sequence-to-Sequence (Seq2Seq)** dengan encoder-decoder menggunakan TensorFlow dan Keras, yang dirancang untuk menangani data gambar dengan prediksi berbasis sekuensial, misalnya untuk mendeteksi emosi wajah dalam bentuk distribusi probabilitas pada 7 kelas emosi.\n",
    "\n",
    "\n",
    "Dibangun dengan arsitektur Convolutional Neural Network (CNN).\n",
    "Tugasnya adalah mengekstraksi fitur dari gambar input (dengan dimensi 48x48x3).\n",
    "CNN menggunakan dua lapisan Conv2D diikuti oleh MaxPooling2D untuk reduksi dimensi.\n",
    "Akhirnya, fitur diratakan (Flatten) dan dilanjutkan dengan lapisan dense untuk menghasilkan representasi fitur latar belakang.\n",
    "Decoder:\n",
    "\n",
    "Dibangun dengan Long Short-Term Memory (LSTM) untuk menghasilkan prediksi sekuensial berbasis keluaran dari encoder.\n",
    "Fitur yang diekstrak oleh encoder direntangkan menjadi sekuens (RepeatVector(seq_len)).\n",
    "LSTM pertama memproses seluruh sekuens, menghasilkan urutan fitur, dan LSTM kedua menyederhanakan representasi sekuensial ini.\n",
    "Lapisan Dense dengan fungsi aktivasi softmax digunakan untuk memprediksi probabilitas dari 7 emosi pada setiap langkah keluaran.\n",
    "Model Seq2Seq:\n",
    "\n",
    "Model ini dibangun sebagai kelas kustom menggunakan Keras, menyatukan encoder dan decoder.\n",
    "Fungsi call mendefinisikan alur data: input diproses oleh encoder, lalu diteruskan ke decoder.\n",
    "Penggunaan GPU:\n",
    "\n",
    "Bagian kode memeriksa GPU yang tersedia dan mengaktifkan alokasi memori dinamis untuk menghindari kesalahan alokasi memori tetap.\n",
    "Pelatihan:\n",
    "\n",
    "Model dilatih menggunakan train_generator dan test_generator, dengan augmentasi data pada gambar input.\n",
    "Optimizer Adam digunakan untuk mempercepat konvergensi.\n",
    "Loss function categorical_crossentropy diterapkan karena targetnya adalah klasifikasi multi-kelas.\n",
    "Evaluasi:\n",
    "\n",
    "Model diuji pada data validasi (test_generator) setelah pelatihan.\n",
    "Akurasi akhir dicetak untuk mengevaluasi performa model pada data uji.\n",
    "Manfaat Arsitektur:\n",
    "Encoder-decoder memungkinkan model untuk menangani hubungan kompleks dalam data.\n",
    "Kombinasi CNN dan LSTM sangat cocok untuk masalah yang membutuhkan pemrosesan data spasial dan sekuensial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e61ba0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 38ms/step - accuracy: 0.2982 - loss: 1.7140 - val_accuracy: 0.4363 - val_loss: 1.4382\n",
      "Epoch 2/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 41ms/step - accuracy: 0.4425 - loss: 1.4358 - val_accuracy: 0.5010 - val_loss: 1.2965\n",
      "Epoch 3/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 40ms/step - accuracy: 0.4905 - loss: 1.3241 - val_accuracy: 0.5224 - val_loss: 1.2599\n",
      "Epoch 4/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 40ms/step - accuracy: 0.5166 - loss: 1.2645 - val_accuracy: 0.5356 - val_loss: 1.2030\n",
      "Epoch 5/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 42ms/step - accuracy: 0.5320 - loss: 1.2184 - val_accuracy: 0.5503 - val_loss: 1.1796\n",
      "Epoch 6/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 40ms/step - accuracy: 0.5509 - loss: 1.1804 - val_accuracy: 0.5595 - val_loss: 1.1598\n",
      "Epoch 7/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 43ms/step - accuracy: 0.5642 - loss: 1.1477 - val_accuracy: 0.5603 - val_loss: 1.1660\n",
      "Epoch 8/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 41ms/step - accuracy: 0.5811 - loss: 1.1090 - val_accuracy: 0.5791 - val_loss: 1.1129\n",
      "Epoch 9/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 40ms/step - accuracy: 0.5891 - loss: 1.0916 - val_accuracy: 0.5826 - val_loss: 1.1055\n",
      "Epoch 10/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 38ms/step - accuracy: 0.5944 - loss: 1.0749 - val_accuracy: 0.5921 - val_loss: 1.0943\n",
      "Epoch 11/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.6026 - loss: 1.0592 - val_accuracy: 0.5984 - val_loss: 1.0734\n",
      "Epoch 12/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 30ms/step - accuracy: 0.6090 - loss: 1.0410 - val_accuracy: 0.6044 - val_loss: 1.0561\n",
      "Epoch 13/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 34ms/step - accuracy: 0.6160 - loss: 1.0292 - val_accuracy: 0.6026 - val_loss: 1.0649\n",
      "Epoch 14/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 33ms/step - accuracy: 0.6199 - loss: 1.0101 - val_accuracy: 0.6179 - val_loss: 1.0255\n",
      "Epoch 15/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.6295 - loss: 0.9973 - val_accuracy: 0.6124 - val_loss: 1.0449\n",
      "Epoch 16/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 31ms/step - accuracy: 0.6341 - loss: 0.9852 - val_accuracy: 0.6168 - val_loss: 1.0322\n",
      "Epoch 17/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 31ms/step - accuracy: 0.6371 - loss: 0.9722 - val_accuracy: 0.6191 - val_loss: 1.0203\n",
      "Epoch 18/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 31ms/step - accuracy: 0.6384 - loss: 0.9675 - val_accuracy: 0.6216 - val_loss: 1.0116\n",
      "Epoch 19/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.6440 - loss: 0.9525 - val_accuracy: 0.6229 - val_loss: 1.0177\n",
      "Epoch 20/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 31ms/step - accuracy: 0.6448 - loss: 0.9483 - val_accuracy: 0.6303 - val_loss: 0.9939\n",
      "Epoch 21/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 31ms/step - accuracy: 0.6519 - loss: 0.9345 - val_accuracy: 0.6191 - val_loss: 1.0255\n",
      "Epoch 22/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 30ms/step - accuracy: 0.6538 - loss: 0.9288 - val_accuracy: 0.6283 - val_loss: 1.0053\n",
      "Epoch 23/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 31ms/step - accuracy: 0.6597 - loss: 0.9198 - val_accuracy: 0.6302 - val_loss: 0.9963\n",
      "Epoch 24/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.6573 - loss: 0.9132 - val_accuracy: 0.6370 - val_loss: 0.9815\n",
      "Epoch 25/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 29ms/step - accuracy: 0.6646 - loss: 0.9004 - val_accuracy: 0.6350 - val_loss: 0.9762\n",
      "Epoch 26/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 30ms/step - accuracy: 0.6704 - loss: 0.8918 - val_accuracy: 0.6410 - val_loss: 0.9812\n",
      "Epoch 27/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 30ms/step - accuracy: 0.6737 - loss: 0.8819 - val_accuracy: 0.6417 - val_loss: 0.9848\n",
      "Epoch 28/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 30ms/step - accuracy: 0.6738 - loss: 0.8748 - val_accuracy: 0.6439 - val_loss: 0.9672\n",
      "Epoch 29/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 31ms/step - accuracy: 0.6808 - loss: 0.8630 - val_accuracy: 0.6488 - val_loss: 0.9560\n",
      "Epoch 30/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 31ms/step - accuracy: 0.6804 - loss: 0.8697 - val_accuracy: 0.6466 - val_loss: 0.9612\n",
      "Epoch 31/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.6811 - loss: 0.8606 - val_accuracy: 0.6441 - val_loss: 0.9662\n",
      "Epoch 32/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.6832 - loss: 0.8536 - val_accuracy: 0.6454 - val_loss: 0.9594\n",
      "Epoch 33/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 32ms/step - accuracy: 0.6839 - loss: 0.8570 - val_accuracy: 0.6411 - val_loss: 0.9766\n",
      "Epoch 34/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 31ms/step - accuracy: 0.6881 - loss: 0.8408 - val_accuracy: 0.6531 - val_loss: 0.9464\n",
      "Epoch 35/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.6945 - loss: 0.8369 - val_accuracy: 0.6519 - val_loss: 0.9429\n",
      "Epoch 36/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.6941 - loss: 0.8314 - val_accuracy: 0.6495 - val_loss: 0.9695\n",
      "Epoch 37/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 31ms/step - accuracy: 0.6945 - loss: 0.8277 - val_accuracy: 0.6619 - val_loss: 0.9286\n",
      "Epoch 38/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 32ms/step - accuracy: 0.6979 - loss: 0.8193 - val_accuracy: 0.6586 - val_loss: 0.9530\n",
      "Epoch 39/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 33ms/step - accuracy: 0.7001 - loss: 0.8120 - val_accuracy: 0.6550 - val_loss: 0.9389\n",
      "Epoch 40/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 33ms/step - accuracy: 0.6969 - loss: 0.8170 - val_accuracy: 0.6525 - val_loss: 0.9492\n",
      "Epoch 41/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 32ms/step - accuracy: 0.7016 - loss: 0.8161 - val_accuracy: 0.6578 - val_loss: 0.9535\n",
      "Epoch 42/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 33ms/step - accuracy: 0.7038 - loss: 0.8029 - val_accuracy: 0.6607 - val_loss: 0.9368\n",
      "Epoch 43/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 34ms/step - accuracy: 0.7056 - loss: 0.8003 - val_accuracy: 0.6646 - val_loss: 0.9329\n",
      "Epoch 44/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 34ms/step - accuracy: 0.7090 - loss: 0.7966 - val_accuracy: 0.6691 - val_loss: 0.9208\n",
      "Epoch 45/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 36ms/step - accuracy: 0.7090 - loss: 0.7902 - val_accuracy: 0.6705 - val_loss: 0.9114\n",
      "Epoch 46/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 34ms/step - accuracy: 0.7110 - loss: 0.7862 - val_accuracy: 0.6641 - val_loss: 0.9275\n",
      "Epoch 47/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 32ms/step - accuracy: 0.7165 - loss: 0.7840 - val_accuracy: 0.6738 - val_loss: 0.9135\n",
      "Epoch 48/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 32ms/step - accuracy: 0.7150 - loss: 0.7789 - val_accuracy: 0.6731 - val_loss: 0.9031\n",
      "Epoch 49/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 29ms/step - accuracy: 0.7147 - loss: 0.7826 - val_accuracy: 0.6634 - val_loss: 0.9223\n",
      "Epoch 50/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 29ms/step - accuracy: 0.7154 - loss: 0.7767 - val_accuracy: 0.6553 - val_loss: 0.9456\n",
      "Epoch 51/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 29ms/step - accuracy: 0.7164 - loss: 0.7757 - val_accuracy: 0.6701 - val_loss: 0.9172\n",
      "Epoch 52/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 29ms/step - accuracy: 0.7161 - loss: 0.7835 - val_accuracy: 0.6766 - val_loss: 0.8967\n",
      "Epoch 53/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 30ms/step - accuracy: 0.7166 - loss: 0.7677 - val_accuracy: 0.6613 - val_loss: 0.9351\n",
      "Epoch 54/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 32ms/step - accuracy: 0.7235 - loss: 0.7640 - val_accuracy: 0.6626 - val_loss: 0.9324\n",
      "Epoch 55/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 31ms/step - accuracy: 0.7246 - loss: 0.7567 - val_accuracy: 0.6646 - val_loss: 0.9289\n",
      "Epoch 56/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 32ms/step - accuracy: 0.7223 - loss: 0.7660 - val_accuracy: 0.6739 - val_loss: 0.9050\n",
      "Epoch 57/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.7248 - loss: 0.7563 - val_accuracy: 0.6761 - val_loss: 0.9046\n",
      "Epoch 58/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.7218 - loss: 0.7637 - val_accuracy: 0.6726 - val_loss: 0.9084\n",
      "Epoch 59/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.7274 - loss: 0.7434 - val_accuracy: 0.6752 - val_loss: 0.9007\n",
      "Epoch 60/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.7290 - loss: 0.7435 - val_accuracy: 0.6719 - val_loss: 0.9018\n",
      "Epoch 61/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.7259 - loss: 0.7448 - val_accuracy: 0.6813 - val_loss: 0.8953\n",
      "Epoch 62/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.7295 - loss: 0.7516 - val_accuracy: 0.6752 - val_loss: 0.8935\n",
      "Epoch 63/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.7291 - loss: 0.7412 - val_accuracy: 0.6778 - val_loss: 0.8988\n",
      "Epoch 64/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.7184 - loss: 0.7736 - val_accuracy: 0.6714 - val_loss: 0.9141\n",
      "Epoch 65/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 30ms/step - accuracy: 0.7295 - loss: 0.7405 - val_accuracy: 0.6634 - val_loss: 0.9265\n",
      "Epoch 66/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 30ms/step - accuracy: 0.7307 - loss: 0.7393 - val_accuracy: 0.6714 - val_loss: 0.9121\n",
      "Epoch 67/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.7337 - loss: 0.7357 - val_accuracy: 0.6731 - val_loss: 0.9091\n",
      "Epoch 68/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 30ms/step - accuracy: 0.7298 - loss: 0.7439 - val_accuracy: 0.6832 - val_loss: 0.8852\n",
      "Epoch 69/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 30ms/step - accuracy: 0.7314 - loss: 0.7351 - val_accuracy: 0.6570 - val_loss: 0.9629\n",
      "Epoch 70/70\n",
      "\u001b[1m1799/1799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 30ms/step - accuracy: 0.7149 - loss: 0.7828 - val_accuracy: 0.6695 - val_loss: 0.9249\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6712 - loss: 0.9120\n",
      "Test accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Constants\n",
    "img_height, img_width = 48, 48\n",
    "num_emotions = 7\n",
    "seq_len = 10\n",
    "\n",
    "def build_encoder():\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(img_height, img_width, 3)),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_decoder(encoder_output_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(encoder_output_shape,)),\n",
    "        layers.RepeatVector(seq_len),\n",
    "        layers.LSTM(64, return_sequences=True),\n",
    "        layers.LSTM(32),\n",
    "        layers.Dense(num_emotions, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "class Seq2Seq(keras.Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Encode input\n",
    "        encoded = self.encoder(inputs)\n",
    "        \n",
    "        # Decode encoded features\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        return decoded\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        print(f\"Using GPU: {gpus[0]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU Configuration Error: {e}\")\n",
    "\n",
    "# Create Model\n",
    "encoder = build_encoder()\n",
    "decoder = build_decoder(encoder.output_shape[1])\n",
    "seq2seq = Seq2Seq(encoder, decoder)\n",
    "\n",
    "# Compile Model\n",
    "seq2seq.compile(\n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "seq2seq.fit(train_generator, epochs=70, validation_data=test_generator)\n",
    "loss, accuracy = seq2seq.evaluate(test_generator)\n",
    "print(f'Test accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6deacb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('emotion_detection_model_seq2seq.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e3442-c18a-4bdd-8133-d9fa10cfea14",
   "metadata": {},
   "source": [
    "**Program untuk mendeteksi emosi wajah secara real-time menggunakan kamera dan dua model deep learning yang telah dilatih sebelumnya (Seq2Seq dan CNN).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "694ed1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the trained models\n",
    "model_v1 = tf.keras.models.load_model('emotion_detection_model_seq2seq.h5')\n",
    "model_v2 = tf.keras.models.load_model('emotion_detection_model_CNN.h5')\n",
    "\n",
    "model_v1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_v2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define the emotions\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral','sad','surprise']\n",
    "\n",
    "# Define the image dimensions\n",
    "img_height, img_width = 48, 48\n",
    "\n",
    "# Open the default camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if the frame is not empty\n",
    "    if not ret:\n",
    "        print(\"Cannot receive frame\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    # Loop through the detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        # Extract the face from the frame\n",
    "        face = gray[y:y+h, x:x+w]\n",
    "\n",
    "        # Resize the face to the input shape of the model\n",
    "        face = cv2.resize(face, (img_height, img_width))\n",
    "\n",
    "        # Convert the face to a 3-channel image\n",
    "        face = cv2.cvtColor(cv2.cvtColor(face, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Expand the dimensions of the face to match the input shape of the model\n",
    "        face = np.expand_dims(face, axis=0)\n",
    "        face = face / 255.0\n",
    "\n",
    "        # Make a prediction on the face using both models\n",
    "        prediction_v1 = model_v1.predict(face)\n",
    "        prediction_v2 = model_v2.predict(face)\n",
    "\n",
    "        # Get the predicted emotions and their probabilities\n",
    "        predicted_emotion_v1 = emotions[np.argmax(prediction_v1)]\n",
    "        predicted_emotion_v2 = emotions[np.argmax(prediction_v2)]\n",
    "\n",
    "        # Display the predicted emotions and their probabilities on the frame\n",
    "        cv2.putText(frame, f\"seq2seq: {predicted_emotion_v1} ({np.max(prediction_v1)*100:.2f}%)\", (x, y-30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 1)\n",
    "        cv2.putText(frame, f\"CNN: {predicted_emotion_v2} ({np.max(prediction_v2)*100:.2f}%)\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 1)\n",
    "\n",
    "        # Display the probabilities of each emotion for both models\n",
    "        for i, emotion in enumerate(emotions):\n",
    "            cv2.putText(frame, f\"{emotion}: {prediction_v1[0][i]*100:.2f}% (seq2seq) / {prediction_v2[0][i]*100:.2f}% (CNN)\", (x, y+h+10+i*15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "\n",
    "    # Exit on press 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d24117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
